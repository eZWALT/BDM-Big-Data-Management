# UNDER NO CIRCUMSTANCES SHOULD THIS FILE BE CHANGED

services:
  minio:
    image: minio/minio
    container_name: minio
    hostname: minio
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data

  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    hostname: spark-master
    restart: unless-stopped
    ports:
      - "8090:8090"  # Spark Web UI
      - "7077:7077"  # Spark Master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8090

  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-1
    hostname: spark-worker-1
    restart: unless-stopped
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_WEBUI_PORT=8091
    depends_on:
      - spark-master


  # To test the Spark cluster, we could run scripts having the host machine be
  # the driver. However, because the containers in the compose are isolated on
  # their own network, the IPs to access the data from the host machine and the
  # workers are different. To solve this, we'll run a Jupyter notebook server
  # inside the Spark cluster. This way, the driver will be inside the same
  # network as the workers.
  spark-notebook:
    image: jupyter/pyspark-notebook:spark-3.5.0
    container_name: spark-notebook
    hostname: spark-notebook
    restart: unless-stopped
    ports:
      - "8888:8888"  # Jupyter Notebook
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_LOCAL_IP=spark-notebook
    depends_on:
      - spark-master
      - spark-worker-1
    volumes:
      - ./test/notebooks:/home/jovyan

volumes:
  minio-data:
    driver: local