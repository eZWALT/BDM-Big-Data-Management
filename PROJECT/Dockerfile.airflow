# Start with the official Apache Airflow image
FROM apache/airflow:2.11.0rc1-python3.11

# Install OpenJDK (Java 17) needed for spark!!!
USER root 
RUN apt-get update \
    && apt-get install -y --no-install-recommends wget \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

RUN apt-get update \
    && apt-get install -y --no-install-recommends openjdk-17-jre-headless \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install spark 3.5.0
RUN wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz \
    && tar -xzf spark-3.5.0-bin-hadoop3.tgz -C /opt/ \
    && rm spark-3.5.0-bin-hadoop3.tgz \
    && ln -s /opt/spark-3.5.0-bin-hadoop3 /opt/spark

RUN apt-get update \
    && apt-get install -y --no-install-recommends procps \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

USER airflow

# Set environment variables for Spark
ENV SPARK_HOME=/opt/spark \
    PATH=$PATH:/opt/spark/bin \
    PYSPARK_PYTHON=python3.11 \
    PYSPARK_DRIVER_PYTHON=python3.11 \
    JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    
# Install the required Python packages
# We need airflow, spark and vadersentiment+duckdb for Consumption zone
RUN pip install --no-cache-dir apache-airflow-providers-apache-spark==5.2.1 --break-system-packages
RUN pip install --no-cache-dir pyspark==3.5.0 --break-system-packages
RUN pip install --no-cache-dir vadersentiment==3.3.2 --break-system-packages
RUN pip install --no-cache-dir duckdb==1.2.2 --break-system-packages